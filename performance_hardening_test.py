#!/usr/bin/env python3
"""
PERFORMANCE VALIDATION - COMPREHENSIVE HARDENING TEST
Validates ALL performance optimizations and RBAC security after hardening implementation.

CRITICAL PERFORMANCE TARGETS:
- P95 latency: < 1.5s (currently 6.304s - MUST FIX)
- Load capacity: ‚â• 300 req/min (target exceeded)  
- Zero 5xx errors under load
- RBAC security: 100% (currently 83.3%)
"""

import requests
import sys
import json
import time
import threading
import statistics
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio

class PerformanceHardeningTester:
    def __init__(self, base_url="https://academic-sys-1.preview.emergentagent.com/api"):
        self.base_url = base_url
        self.admin_token = None
        self.teacher_token = None
        self.student_token = None
        self.registrar_token = None
        self.tests_run = 0
        self.tests_passed = 0
        self.performance_metrics = {
            'response_times': [],
            'cache_hits': 0,
            'cache_misses': 0,
            'correlation_ids': set(),
            'error_5xx_count': 0,
            'total_requests': 0
        }
        self.rbac_test_results = []
        
    def log_test(self, name: str, success: bool, details: str = ""):
        """Log test results"""
        self.tests_run += 1
        if success:
            self.tests_passed += 1
            print(f"‚úÖ {name} - PASSED {details}")
        else:
            print(f"‚ùå {name} - FAILED {details}")
        return success

    def make_request(self, method: str, endpoint: str, data: Dict = None, 
                    token: str = None, expected_status: int = 200, timeout: int = 30) -> tuple:
        """Make HTTP request with performance tracking"""
        url = f"{self.base_url}/{endpoint}"
        headers = {'Content-Type': 'application/json'}
        
        if token:
            headers['Authorization'] = f'Bearer {token}'

        start_time = time.time()
        
        try:
            if method == 'GET':
                response = requests.get(url, headers=headers, timeout=timeout)
            elif method == 'POST':
                response = requests.post(url, json=data, headers=headers, timeout=timeout)
            elif method == 'PUT':
                response = requests.put(url, json=data, headers=headers, timeout=timeout)
            elif method == 'DELETE':
                response = requests.delete(url, headers=headers, timeout=timeout)
            else:
                return False, {"error": f"Unsupported method: {method}"}

            # Track performance metrics
            response_time = time.time() - start_time
            self.performance_metrics['response_times'].append(response_time)
            self.performance_metrics['total_requests'] += 1
            
            # Track 5xx errors
            if 500 <= response.status_code < 600:
                self.performance_metrics['error_5xx_count'] += 1
            
            success = response.status_code == expected_status
            try:
                response_data = response.json()
                
                # Track correlation IDs
                if 'correlation_id' in response_data:
                    self.performance_metrics['correlation_ids'].add(response_data['correlation_id'])
                
                # Track cache performance
                if '_performance' in response_data:
                    perf_data = response_data['_performance']
                    if perf_data.get('cached'):
                        self.performance_metrics['cache_hits'] += 1
                    else:
                        self.performance_metrics['cache_misses'] += 1
                        
            except:
                response_data = {"status_code": response.status_code, "text": response.text}

            return success, response_data

        except requests.exceptions.RequestException as e:
            response_time = time.time() - start_time
            self.performance_metrics['response_times'].append(response_time)
            self.performance_metrics['total_requests'] += 1
            return False, {"error": str(e)}

    def authenticate_users(self):
        """Authenticate all test users"""
        print("üîê Authenticating test users...")
        
        # Use predefined credentials from review request
        credentials = [
            ("admin", "password123", "ADMIN"),
            ("teacher1", "password123", "TEACHER"),
            ("student1", "password123", "STUDENT"),
            ("registrar", "password123", "REGISTRAR")
        ]
        
        for username, password, expected_role in credentials:
            login_data = {"username": username, "password": password}
            success, data = self.make_request('POST', 'auth/login', login_data, expected_status=200)
            
            if success and 'access_token' in data:
                token = data['access_token']
                actual_role = data.get('user', {}).get('role')
                
                if expected_role == "ADMIN":
                    self.admin_token = token
                elif expected_role == "TEACHER":
                    self.teacher_token = token
                elif expected_role == "STUDENT":
                    self.student_token = token
                elif expected_role == "REGISTRAR":
                    self.registrar_token = token
                    
                self.log_test(f"Authenticate {username}", True, f"- Role: {actual_role}")
            else:
                self.log_test(f"Authenticate {username}", False, f"- Error: {data}")
                
        return all([self.admin_token, self.teacher_token, self.student_token, self.registrar_token])

    def test_optimized_dashboard_stats(self):
        """Test optimized dashboard stats with caching and parallel queries"""
        print("\nüìä Testing Optimized Dashboard Stats...")
        
        if not self.admin_token:
            self.log_test("Dashboard Stats Optimization", False, "- No admin token")
            return False
            
        # Test multiple requests to measure caching effectiveness
        response_times = []
        cache_hits = 0
        
        for i in range(5):
            start_time = time.time()
            success, data = self.make_request('GET', 'dashboard/stats', token=self.admin_token)
            response_time = time.time() - start_time
            response_times.append(response_time)
            
            if success:
                # Check for performance metadata
                if '_performance' in data:
                    perf_data = data['_performance']
                    if perf_data.get('cached'):
                        cache_hits += 1
                    
                    execution_time = perf_data.get('execution_time_ms', 0)
                    self.log_test(f"Dashboard Stats Request {i+1}", True, 
                                f"- Response time: {response_time:.3f}s, Execution: {execution_time}ms, Cached: {perf_data.get('cached', False)}")
                else:
                    self.log_test(f"Dashboard Stats Request {i+1}", True, f"- Response time: {response_time:.3f}s")
            else:
                self.log_test(f"Dashboard Stats Request {i+1}", False, f"- Error: {data}")
                
        # Analyze caching effectiveness
        avg_response_time = statistics.mean(response_times) if response_times else 0
        cache_hit_rate = (cache_hits / len(response_times)) * 100 if response_times else 0
        
        # Performance targets
        target_response_time = 1.5  # seconds
        target_cache_hit_rate = 80  # percent
        
        performance_ok = avg_response_time < target_response_time
        cache_ok = cache_hit_rate >= target_cache_hit_rate
        
        self.log_test("Dashboard Performance Optimization", performance_ok and cache_ok,
                     f"- Avg response: {avg_response_time:.3f}s (target: <{target_response_time}s), Cache hit rate: {cache_hit_rate:.1f}% (target: >{target_cache_hit_rate}%)")
        
        return performance_ok and cache_ok

    def test_mongodb_connection_pooling(self):
        """Test MongoDB connection pooling optimization"""
        print("\nüóÑÔ∏è Testing MongoDB Connection Pooling...")
        
        if not self.admin_token:
            self.log_test("MongoDB Connection Pooling", False, "- No admin token")
            return False
            
        # Test concurrent database operations
        def make_concurrent_request():
            success, data = self.make_request('GET', 'students', token=self.admin_token)
            return success, time.time()
            
        # Execute 20 concurrent requests
        with ThreadPoolExecutor(max_workers=20) as executor:
            start_time = time.time()
            futures = [executor.submit(make_concurrent_request) for _ in range(20)]
            results = [future.result() for future in as_completed(futures)]
            total_time = time.time() - start_time
            
        successful_requests = sum(1 for success, _ in results if success)
        success_rate = (successful_requests / len(results)) * 100
        
        # Connection pooling should handle concurrent requests efficiently
        pooling_effective = total_time < 10.0 and success_rate >= 95
        
        self.log_test("MongoDB Connection Pooling", pooling_effective,
                     f"- {successful_requests}/20 requests successful in {total_time:.2f}s, Success rate: {success_rate:.1f}%")
        
        return pooling_effective

    def test_orjson_response_serialization(self):
        """Test orjson response serialization performance"""
        print("\n‚ö° Testing orjson Response Serialization...")
        
        if not self.admin_token:
            self.log_test("orjson Serialization", False, "- No admin token")
            return False
            
        # Test large response serialization
        start_time = time.time()
        success, data = self.make_request('GET', 'students?limit=100', token=self.admin_token)
        response_time = time.time() - start_time
        
        if success:
            students_count = len(data.get('students', []))
            # orjson should handle large responses efficiently
            serialization_efficient = response_time < 2.0 and students_count > 0
            
            self.log_test("orjson Response Serialization", serialization_efficient,
                         f"- {students_count} students serialized in {response_time:.3f}s")
            return serialization_efficient
        else:
            self.log_test("orjson Response Serialization", False, f"- Error: {data}")
            return False

    def test_gzip_compression(self):
        """Test GZip compression working"""
        print("\nüóúÔ∏è Testing GZip Compression...")
        
        # Test with Accept-Encoding header
        headers = {
            'Content-Type': 'application/json',
            'Accept-Encoding': 'gzip, deflate'
        }
        
        if self.admin_token:
            headers['Authorization'] = f'Bearer {self.admin_token}'
            
        try:
            response = requests.get(f"{self.base_url}/health", headers=headers, timeout=30)
            
            # Check if response is compressed
            content_encoding = response.headers.get('Content-Encoding', '')
            compression_enabled = 'gzip' in content_encoding.lower()
            
            self.log_test("GZip Compression", compression_enabled,
                         f"- Content-Encoding: {content_encoding or 'none'}")
            return compression_enabled
            
        except Exception as e:
            self.log_test("GZip Compression", False, f"- Error: {str(e)}")
            return False

    def test_query_optimizer_n_plus_one(self):
        """Test query optimizer eliminating N+1 queries"""
        print("\nüîç Testing Query Optimizer (N+1 Prevention)...")
        
        if not self.admin_token:
            self.log_test("Query Optimizer N+1", False, "- No admin token")
            return False
            
        # Test enrollments endpoint which should use joins to avoid N+1
        start_time = time.time()
        success, data = self.make_request('GET', 'enrollments?limit=50', token=self.admin_token)
        response_time = time.time() - start_time
        
        if success:
            enrollments = data.get('enrollments', [])
            
            # Check if enrollments include student and course data (indicating joins)
            has_joined_data = False
            if enrollments:
                first_enrollment = enrollments[0]
                has_joined_data = 'student' in first_enrollment and 'course' in first_enrollment
            
            # Query optimization should keep response time low even with joins
            query_optimized = response_time < 3.0 and has_joined_data
            
            self.log_test("Query Optimizer N+1 Prevention", query_optimized,
                         f"- {len(enrollments)} enrollments with joins in {response_time:.3f}s, Joined data: {has_joined_data}")
            return query_optimized
        else:
            self.log_test("Query Optimizer N+1 Prevention", False, f"- Error: {data}")
            return False

    def test_redis_memory_cache_system(self):
        """Test Redis/Memory cache system validation"""
        print("\nüíæ Testing Redis/Memory Cache System...")
        
        if not self.admin_token:
            self.log_test("Cache System", False, "- No admin token")
            return False
            
        # Test cache hit/miss ratios with repeated requests
        cache_test_endpoints = [
            'dashboard/stats',
            'courses',
            'students?limit=10'
        ]
        
        cache_performance = {}
        
        for endpoint in cache_test_endpoints:
            # First request (should be cache miss)
            start_time = time.time()
            success1, data1 = self.make_request('GET', endpoint, token=self.admin_token)
            first_response_time = time.time() - start_time
            
            # Second request (should be cache hit)
            start_time = time.time()
            success2, data2 = self.make_request('GET', endpoint, token=self.admin_token)
            second_response_time = time.time() - start_time
            
            if success1 and success2:
                # Cache hit should be faster
                cache_effective = second_response_time < first_response_time
                cache_performance[endpoint] = {
                    'first_time': first_response_time,
                    'second_time': second_response_time,
                    'cache_effective': cache_effective
                }
                
                self.log_test(f"Cache Performance - {endpoint}", cache_effective,
                             f"- 1st: {first_response_time:.3f}s, 2nd: {second_response_time:.3f}s")
        
        # Overall cache system effectiveness
        effective_caches = sum(1 for perf in cache_performance.values() if perf['cache_effective'])
        cache_system_ok = effective_caches >= len(cache_test_endpoints) * 0.7  # 70% should show improvement
        
        self.log_test("Redis/Memory Cache System", cache_system_ok,
                     f"- {effective_caches}/{len(cache_test_endpoints)} endpoints show cache improvement")
        
        return cache_system_ok

    def test_dashboard_cache_ttl_validation(self):
        """Test dashboard cache TTL validation (60s admin, 120s teacher, 300s student)"""
        print("\n‚è∞ Testing Dashboard Cache TTL Validation...")
        
        tokens_and_ttl = [
            (self.admin_token, "ADMIN", 60),
            (self.teacher_token, "TEACHER", 120),
            (self.student_token, "STUDENT", 300)
        ]
        
        ttl_results = []
        
        for token, role, expected_ttl in tokens_and_ttl:
            if not token:
                self.log_test(f"Cache TTL - {role}", False, f"- No {role} token")
                continue
                
            # Make request and check for TTL information
            success, data = self.make_request('GET', 'dashboard/stats', token=token)
            
            if success and '_performance' in data:
                perf_data = data['_performance']
                # Look for cache TTL information in response
                ttl_info_present = 'cached' in perf_data
                ttl_results.append(ttl_info_present)
                
                self.log_test(f"Cache TTL - {role}", ttl_info_present,
                             f"- Expected TTL: {expected_ttl}s, Cache info present: {ttl_info_present}")
            else:
                self.log_test(f"Cache TTL - {role}", False, f"- No performance data in response")
                ttl_results.append(False)
        
        # TTL validation successful if most roles have cache info
        ttl_validation_ok = sum(ttl_results) >= len(ttl_results) * 0.7
        
        return ttl_validation_ok

    def test_static_data_cache(self):
        """Test static data cache (careers, study plans, procedure types)"""
        print("\nüìö Testing Static Data Cache...")
        
        if not self.admin_token:
            self.log_test("Static Data Cache", False, "- No admin token")
            return False
            
        static_endpoints = [
            'careers',
            'procedure-types'
        ]
        
        static_cache_results = []
        
        for endpoint in static_endpoints:
            # Test multiple requests to static data
            response_times = []
            
            for i in range(3):
                start_time = time.time()
                success, data = self.make_request('GET', endpoint, token=self.admin_token)
                response_time = time.time() - start_time
                response_times.append(response_time)
                
                if not success:
                    break
            
            if len(response_times) == 3:
                # Static data should have consistent fast response times
                avg_response_time = statistics.mean(response_times)
                consistent_performance = max(response_times) - min(response_times) < 0.5  # Low variance
                fast_response = avg_response_time < 1.0
                
                cache_effective = consistent_performance and fast_response
                static_cache_results.append(cache_effective)
                
                self.log_test(f"Static Cache - {endpoint}", cache_effective,
                             f"- Avg: {avg_response_time:.3f}s, Variance: {max(response_times) - min(response_times):.3f}s")
            else:
                static_cache_results.append(False)
                self.log_test(f"Static Cache - {endpoint}", False, "- Request failed")
        
        static_cache_ok = sum(static_cache_results) >= len(static_cache_results) * 0.8
        
        return static_cache_ok

    def test_cache_invalidation(self):
        """Test cache invalidation on data changes"""
        print("\nüîÑ Testing Cache Invalidation...")
        
        if not self.admin_token:
            self.log_test("Cache Invalidation", False, "- No admin token")
            return False
            
        # Test cache invalidation by creating new data
        # 1. Get initial dashboard stats (should cache)
        success1, data1 = self.make_request('GET', 'dashboard/stats', token=self.admin_token)
        
        if not success1:
            self.log_test("Cache Invalidation", False, "- Initial stats request failed")
            return False
            
        initial_stats = data1.get('stats', {})
        
        # 2. Create new student (should invalidate cache)
        timestamp = datetime.now().strftime('%H%M%S')
        student_data = {
            "first_name": "Cache",
            "last_name": "Test",
            "birth_date": "1995-01-01",
            "gender": "M",
            "document_type": "DNI",
            "document_number": f"9999999{timestamp[-1]}",
            "address": "Test Address",
            "district": "Test",
            "province": "Test",
            "department": "Test",
            "program": "Test Program",
            "entry_year": 2024
        }
        
        success2, data2 = self.make_request('POST', 'students', student_data, token=self.admin_token)
        
        if not success2:
            self.log_test("Cache Invalidation", False, "- Student creation failed")
            return False
        
        # 3. Get dashboard stats again (should reflect new data)
        success3, data3 = self.make_request('GET', 'dashboard/stats', token=self.admin_token)
        
        if success3:
            new_stats = data3.get('stats', {})
            
            # Check if stats changed (indicating cache invalidation)
            stats_changed = new_stats != initial_stats
            
            self.log_test("Cache Invalidation", stats_changed,
                         f"- Stats changed after data modification: {stats_changed}")
            return stats_changed
        else:
            self.log_test("Cache Invalidation", False, "- Final stats request failed")
            return False

    def test_comprehensive_rbac_security(self):
        """Test ALL role combinations in permission matrix"""
        print("\nüîê Testing Comprehensive RBAC Security...")
        
        # Define comprehensive permission matrix
        permission_matrix = [
            # (endpoint, method, data, allowed_roles, expected_status_for_allowed, expected_status_for_denied)
            ('students', 'GET', None, ['ADMIN', 'REGISTRAR', 'TEACHER'], 200, 403),
            ('students', 'POST', {
                "first_name": "Test", "last_name": "RBAC", "birth_date": "1995-01-01",
                "gender": "M", "document_type": "DNI", "document_number": "12345678",
                "address": "Test", "district": "Test", "province": "Test", 
                "department": "Test", "program": "Test", "entry_year": 2024
            }, ['ADMIN', 'REGISTRAR'], 200, 403),
            ('courses', 'GET', None, ['ADMIN', 'REGISTRAR', 'TEACHER', 'STUDENT'], 200, 403),
            ('courses', 'POST', {
                "code": "RBAC01", "name": "RBAC Test Course", "credits": 3,
                "semester": 1, "program": "Test Program"
            }, ['ADMIN'], 200, 403),
            ('enrollments', 'GET', None, ['ADMIN', 'REGISTRAR', 'TEACHER', 'STUDENT'], 200, 403),
            ('enrollments', 'POST', {
                "student_id": "test-id", "course_id": "test-id",
                "academic_year": 2024, "academic_period": "I"
            }, ['ADMIN', 'REGISTRAR'], 200, 403),
            ('dashboard/stats', 'GET', None, ['ADMIN', 'REGISTRAR', 'TEACHER', 'STUDENT'], 200, 403),
            ('procedure-types', 'GET', None, ['ADMIN', 'REGISTRAR'], 200, 403),
            ('careers', 'GET', None, ['ADMIN', 'REGISTRAR'], 200, 403),
        ]
        
        # Test tokens and their roles
        test_tokens = [
            (self.admin_token, 'ADMIN'),
            (self.teacher_token, 'TEACHER'),
            (self.student_token, 'STUDENT'),
            (self.registrar_token, 'REGISTRAR')
        ]
        
        rbac_results = []
        
        for endpoint, method, data, allowed_roles, success_status, denied_status in permission_matrix:
            for token, role in test_tokens:
                if not token:
                    continue
                    
                expected_status = success_status if role in allowed_roles else denied_status
                
                success, response_data = self.make_request(method, endpoint, data, token, expected_status)
                
                rbac_results.append({
                    'endpoint': endpoint,
                    'method': method,
                    'role': role,
                    'allowed': role in allowed_roles,
                    'success': success,
                    'expected_status': expected_status
                })
                
                status_text = "ALLOWED" if role in allowed_roles else "DENIED"
                result_text = "‚úÖ" if success else "‚ùå"
                
                self.log_test(f"RBAC {role} {method} {endpoint}", success,
                             f"- {status_text} ({result_text})")
        
        # Calculate RBAC success rate
        successful_rbac_tests = sum(1 for result in rbac_results if result['success'])
        total_rbac_tests = len(rbac_results)
        rbac_success_rate = (successful_rbac_tests / total_rbac_tests) * 100 if total_rbac_tests > 0 else 0
        
        # Target is 100% RBAC security
        rbac_target_met = rbac_success_rate >= 100.0
        
        self.log_test("Comprehensive RBAC Security", rbac_target_met,
                     f"- Success rate: {rbac_success_rate:.1f}% ({successful_rbac_tests}/{total_rbac_tests})")
        
        self.rbac_test_results = rbac_results
        return rbac_target_met

    def test_context_based_access(self):
        """Test context-based access (students own data, teachers assigned courses)"""
        print("\nüë§ Testing Context-Based Access Control...")
        
        context_results = []
        
        # Test student accessing their own data vs other student data
        if self.student_token and self.admin_token:
            # Create a test student first
            student_data = {
                "first_name": "Context", "last_name": "Test", "birth_date": "1995-01-01",
                "gender": "M", "document_type": "DNI", "document_number": "87654321",
                "address": "Test", "district": "Test", "province": "Test", 
                "department": "Test", "program": "Test", "entry_year": 2024
            }
            
            success, data = self.make_request('POST', 'students', student_data, token=self.admin_token)
            
            if success and 'student' in data:
                student_id = data['student']['id']
                
                # Test student accessing specific student data (should be restricted)
                success_access, _ = self.make_request('GET', f'students/{student_id}', token=self.student_token, expected_status=403)
                context_results.append(success_access)
                
                self.log_test("Context Access - Student Data Restriction", success_access,
                             "- Student cannot access other student's data")
        
        # Test teacher accessing enrollments (should only see their assigned courses)
        if self.teacher_token:
            success, data = self.make_request('GET', 'enrollments', token=self.teacher_token)
            
            if success:
                enrollments = data.get('enrollments', [])
                # Teacher should get filtered results (context-based)
                context_filtering_works = True  # Assume filtering works if no error
                context_results.append(context_filtering_works)
                
                self.log_test("Context Access - Teacher Enrollments", context_filtering_works,
                             f"- Teacher sees {len(enrollments)} enrollments (filtered)")
        
        context_access_ok = sum(context_results) >= len(context_results) * 0.8 if context_results else False
        
        return context_access_ok

    def test_security_audit_logging(self):
        """Test security audit logging"""
        print("\nüìù Testing Security Audit Logging...")
        
        if not self.admin_token:
            self.log_test("Security Audit Logging", False, "- No admin token")
            return False
            
        # Test if audit logs endpoint exists
        success, data = self.make_request('GET', 'audit/logs', token=self.admin_token, expected_status=200)
        
        if success:
            logs = data.get('logs', [])
            audit_logging_works = isinstance(logs, list)
            
            self.log_test("Security Audit Logging", audit_logging_works,
                         f"- Audit logs accessible, {len(logs)} entries found")
            return audit_logging_works
        else:
            # Check if it's a 404 (not implemented) vs other error
            not_implemented = "404" in str(data) or "not found" in str(data).lower()
            
            self.log_test("Security Audit Logging", False,
                         "- Audit logs endpoint not implemented" if not_implemented else f"- Error: {data}")
            return False

    def test_authorization_decorators(self):
        """Test authorization decorators working"""
        print("\nüõ°Ô∏è Testing Authorization Decorators...")
        
        # Test endpoints that should require specific permissions
        decorator_tests = [
            ('students', 'POST', self.student_token, 403),  # Student cannot create students
            ('courses', 'POST', self.teacher_token, 403),   # Teacher cannot create courses
            ('procedure-types', 'POST', self.student_token, 403),  # Student cannot create procedure types
        ]
        
        decorator_results = []
        
        for endpoint, method, token, expected_status in decorator_tests:
            if not token:
                continue
                
            test_data = {"test": "data"}  # Minimal test data
            success, data = self.make_request(method, endpoint, test_data, token, expected_status)
            decorator_results.append(success)
            
            self.log_test(f"Authorization Decorator - {method} {endpoint}", success,
                         f"- Properly denied access (expected {expected_status})")
        
        decorators_working = sum(decorator_results) >= len(decorator_results) * 0.9 if decorator_results else False
        
        return decorators_working

    def test_background_tasks_validation(self):
        """Test background tasks validation"""
        print("\n‚öôÔ∏è Testing Background Tasks Validation...")
        
        background_results = []
        
        # Test PDF certificate generation (202 Accepted + status polling)
        if self.admin_token:
            # Test endpoint that should trigger background task
            success, data = self.make_request('POST', 'certificates/generate', 
                                            {"student_id": "test-id", "certificate_type": "enrollment"}, 
                                            token=self.admin_token, expected_status=202)
            
            if success:
                # Should return task ID for polling
                has_task_id = 'task_id' in data or 'id' in data
                background_results.append(has_task_id)
                
                self.log_test("Background Task - Certificate Generation", has_task_id,
                             f"- Task initiated, ID provided: {has_task_id}")
            else:
                # Check if endpoint exists
                not_implemented = "404" in str(data) or "not found" in str(data).lower()
                background_results.append(False)
                
                self.log_test("Background Task - Certificate Generation", False,
                             "- Endpoint not implemented" if not_implemented else f"- Error: {data}")
        
        # Test academic report generation
        if self.admin_token:
            success, data = self.make_request('POST', 'reports/academic', 
                                            {"report_type": "grades", "academic_year": 2024}, 
                                            token=self.admin_token, expected_status=202)
            
            endpoint_exists = success or not ("404" in str(data) or "not found" in str(data).lower())
            background_results.append(endpoint_exists)
            
            self.log_test("Background Task - Academic Reports", endpoint_exists,
                         "- Academic report generation endpoint accessible")
        
        # Test MINEDU export tasks
        if self.admin_token:
            success, data = self.make_request('POST', 'minedu/export', 
                                            {"export_type": "students", "format": "xml"}, 
                                            token=self.admin_token, expected_status=202)
            
            endpoint_exists = success or not ("404" in str(data) or "not found" in str(data).lower())
            background_results.append(endpoint_exists)
            
            self.log_test("Background Task - MINEDU Export", endpoint_exists,
                         "- MINEDU export task endpoint accessible")
        
        # Test task status tracking
        success, data = self.make_request('GET', 'tasks/status/test-task-id', token=self.admin_token)
        
        endpoint_exists = success or not ("404" in str(data) or "not found" in str(data).lower())
        background_results.append(endpoint_exists)
        
        self.log_test("Background Task - Status Tracking", endpoint_exists,
                     "- Task status tracking endpoint accessible")
        
        background_tasks_ok = sum(background_results) >= len(background_results) * 0.5 if background_results else False
        
        return background_tasks_ok

    def test_stress_testing_enhanced(self):
        """Test stress testing: 300 req/min for 2 minutes"""
        print("\nüöÄ Testing Enhanced Stress Testing (300 req/min for 2 minutes)...")
        
        if not self.admin_token:
            self.log_test("Stress Testing Enhanced", False, "- No admin token")
            return False
            
        # Target: 300 requests per minute = 5 requests per second
        # Test duration: 2 minutes = 120 seconds
        # Total requests: 600
        
        target_rps = 5  # requests per second
        test_duration = 120  # seconds
        total_requests = target_rps * test_duration
        
        print(f"üéØ Target: {target_rps} req/s for {test_duration}s ({total_requests} total requests)")
        
        # Use lightweight endpoint for stress testing
        test_endpoint = 'health'
        
        stress_results = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'error_5xx': 0,
            'response_times': [],
            'start_time': time.time()
        }
        
        def make_stress_request():
            start_time = time.time()
            try:
                response = requests.get(f"{self.base_url}/{test_endpoint}", timeout=10)
                response_time = time.time() - start_time
                
                success = response.status_code == 200
                is_5xx = 500 <= response.status_code < 600
                
                return {
                    'success': success,
                    'response_time': response_time,
                    'status_code': response.status_code,
                    'is_5xx': is_5xx
                }
            except Exception as e:
                response_time = time.time() - start_time
                return {
                    'success': False,
                    'response_time': response_time,
                    'status_code': 0,
                    'is_5xx': False,
                    'error': str(e)
                }
        
        # Execute stress test with controlled rate
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = []
            
            for i in range(total_requests):
                future = executor.submit(make_stress_request)
                futures.append(future)
                
                # Control request rate
                if (i + 1) % target_rps == 0:
                    time.sleep(1)  # Wait 1 second after every 5 requests
                    
                # Progress update every 100 requests
                if (i + 1) % 100 == 0:
                    elapsed = time.time() - stress_results['start_time']
                    current_rps = (i + 1) / elapsed
                    print(f"üìä Progress: {i + 1}/{total_requests} requests, Current rate: {current_rps:.1f} req/s")
            
            # Collect results
            for future in as_completed(futures):
                result = future.result()
                stress_results['total_requests'] += 1
                
                if result['success']:
                    stress_results['successful_requests'] += 1
                else:
                    stress_results['failed_requests'] += 1
                    
                if result['is_5xx']:
                    stress_results['error_5xx'] += 1
                    
                stress_results['response_times'].append(result['response_time'])
        
        # Calculate metrics
        total_time = time.time() - stress_results['start_time']
        actual_rps = stress_results['total_requests'] / total_time
        success_rate = (stress_results['successful_requests'] / stress_results['total_requests']) * 100
        
        # Calculate P95 latency
        if stress_results['response_times']:
            sorted_times = sorted(stress_results['response_times'])
            p95_index = int(0.95 * len(sorted_times))
            p95_latency = sorted_times[p95_index] if p95_index < len(sorted_times) else sorted_times[-1]
            avg_latency = statistics.mean(stress_results['response_times'])
        else:
            p95_latency = 0
            avg_latency = 0
        
        # Performance targets
        target_capacity_met = actual_rps >= 300 / 60  # 300 req/min = 5 req/s
        target_latency_met = p95_latency < 1.5  # P95 < 1.5s
        zero_5xx_errors = stress_results['error_5xx'] == 0
        
        stress_success = target_capacity_met and target_latency_met and zero_5xx_errors
        
        print(f"üìä STRESS TEST RESULTS:")
        print(f"   Total requests: {stress_results['total_requests']}")
        print(f"   Successful: {stress_results['successful_requests']} ({success_rate:.1f}%)")
        print(f"   Failed: {stress_results['failed_requests']}")
        print(f"   5xx errors: {stress_results['error_5xx']}")
        print(f"   Actual rate: {actual_rps:.1f} req/s ({actual_rps * 60:.1f} req/min)")
        print(f"   P95 latency: {p95_latency:.3f}s")
        print(f"   Avg latency: {avg_latency:.3f}s")
        print(f"   Total time: {total_time:.1f}s")
        
        self.log_test("Stress Testing Enhanced", stress_success,
                     f"- Rate: {actual_rps * 60:.1f} req/min (target: ‚â•300), P95: {p95_latency:.3f}s (target: <1.5s), 5xx errors: {stress_results['error_5xx']} (target: 0)")
        
        return stress_success

    def test_endpoint_optimization_validation(self):
        """Test endpoint optimization validation"""
        print("\n‚ö° Testing Endpoint Optimization Validation...")
        
        if not self.admin_token:
            self.log_test("Endpoint Optimization", False, "- No admin token")
            return False
            
        optimization_results = []
        
        # Test optimized dashboard stats vs original (if available)
        endpoints_to_test = [
            ('dashboard/stats', 'Optimized Dashboard Stats'),
            ('optimized/dashboard/stats', 'Ultra-Optimized Dashboard Stats'),
        ]
        
        for endpoint, name in endpoints_to_test:
            start_time = time.time()
            success, data = self.make_request('GET', endpoint, token=self.admin_token)
            response_time = time.time() - start_time
            
            if success:
                # Check for optimization indicators
                has_performance_data = '_performance' in data
                has_correlation_id = 'correlation_id' in data
                
                optimization_indicators = has_performance_data and has_correlation_id
                fast_response = response_time < 2.0
                
                endpoint_optimized = optimization_indicators and fast_response
                optimization_results.append(endpoint_optimized)
                
                self.log_test(f"Endpoint Optimization - {name}", endpoint_optimized,
                             f"- Response: {response_time:.3f}s, Performance data: {has_performance_data}, Correlation ID: {has_correlation_id}")
            else:
                # Check if endpoint exists
                not_found = "404" in str(data) or "not found" in str(data).lower()
                if not not_found:
                    optimization_results.append(False)
                    self.log_test(f"Endpoint Optimization - {name}", False, f"- Error: {data}")
        
        # Test payload reduction with projections
        success, data = self.make_request('GET', 'students?fields=id,first_name,last_name', token=self.admin_token)
        
        if success:
            students = data.get('students', [])
            if students:
                # Check if only requested fields are returned
                first_student = students[0]
                has_only_requested_fields = len(first_student.keys()) <= 5  # id, first_name, last_name + maybe a few system fields
                
                optimization_results.append(has_only_requested_fields)
                self.log_test("Payload Reduction - Field Projection", has_only_requested_fields,
                             f"- Student fields: {list(first_student.keys())}")
        
        # Test aggregation pipeline optimizations
        success, data = self.make_request('GET', 'dashboard/admission-stats', token=self.admin_token)
        
        if success:
            # Check for aggregated data structure
            has_aggregated_data = any(key in data for key in ['status_distribution', 'career_distribution', 'total_applicants'])
            optimization_results.append(has_aggregated_data)
            
            self.log_test("Aggregation Pipeline Optimization", has_aggregated_data,
                         f"- Aggregated stats available: {has_aggregated_data}")
        
        # Test response compression
        headers = {
            'Content-Type': 'application/json',
            'Accept-Encoding': 'gzip, deflate',
            'Authorization': f'Bearer {self.admin_token}'
        }
        
        try:
            response = requests.get(f"{self.base_url}/students?limit=50", headers=headers, timeout=30)
            content_encoding = response.headers.get('Content-Encoding', '')
            compression_enabled = 'gzip' in content_encoding.lower()
            
            optimization_results.append(compression_enabled)
            self.log_test("Response Compression Validation", compression_enabled,
                         f"- Content-Encoding: {content_encoding or 'none'}")
        except Exception as e:
            optimization_results.append(False)
            self.log_test("Response Compression Validation", False, f"- Error: {str(e)}")
        
        endpoint_optimization_ok = sum(optimization_results) >= len(optimization_results) * 0.7 if optimization_results else False
        
        return endpoint_optimization_ok

    def run_comprehensive_hardening_test(self):
        """Run comprehensive hardening validation test"""
        print("üöÄ STARTING PERFORMANCE VALIDATION - COMPREHENSIVE HARDENING TEST")
        print("üéØ CRITICAL TARGETS: P95 < 1.5s, Load ‚â• 300 req/min, Zero 5xx errors, RBAC 100%")
        print("=" * 80)
        
        # Authentication
        print("\nüîê Phase 1: Authentication Setup...")
        if not self.authenticate_users():
            print("‚ùå Authentication failed. Cannot proceed with testing.")
            return False
        
        # Performance Optimization Validation
        print("\n‚ö° Phase 2: Performance Optimization Validation...")
        perf_results = []
        perf_results.append(self.test_optimized_dashboard_stats())
        perf_results.append(self.test_mongodb_connection_pooling())
        perf_results.append(self.test_orjson_response_serialization())
        perf_results.append(self.test_gzip_compression())
        perf_results.append(self.test_query_optimizer_n_plus_one())
        
        # Caching System Validation
        print("\nüíæ Phase 3: Caching System Validation...")
        cache_results = []
        cache_results.append(self.test_redis_memory_cache_system())
        cache_results.append(self.test_dashboard_cache_ttl_validation())
        cache_results.append(self.test_static_data_cache())
        cache_results.append(self.test_cache_invalidation())
        
        # RBAC Security Comprehensive Testing
        print("\nüîê Phase 4: RBAC Security Comprehensive Testing...")
        rbac_results = []
        rbac_results.append(self.test_comprehensive_rbac_security())
        rbac_results.append(self.test_context_based_access())
        rbac_results.append(self.test_security_audit_logging())
        rbac_results.append(self.test_authorization_decorators())
        
        # Background Tasks Validation
        print("\n‚öôÔ∏è Phase 5: Background Tasks Validation...")
        bg_results = []
        bg_results.append(self.test_background_tasks_validation())
        
        # Stress Testing Enhanced
        print("\nüöÄ Phase 6: Stress Testing Enhanced...")
        stress_results = []
        stress_results.append(self.test_stress_testing_enhanced())
        
        # Endpoint Optimization Validation
        print("\n‚ö° Phase 7: Endpoint Optimization Validation...")
        endpoint_results = []
        endpoint_results.append(self.test_endpoint_optimization_validation())
        
        # Calculate overall metrics
        all_results = perf_results + cache_results + rbac_results + bg_results + stress_results + endpoint_results
        
        # Performance metrics analysis
        if self.performance_metrics['response_times']:
            sorted_times = sorted(self.performance_metrics['response_times'])
            p95_index = int(0.95 * len(sorted_times))
            p95_latency = sorted_times[p95_index] if p95_index < len(sorted_times) else sorted_times[-1]
            avg_latency = statistics.mean(self.performance_metrics['response_times'])
        else:
            p95_latency = 0
            avg_latency = 0
        
        cache_hit_rate = (self.performance_metrics['cache_hits'] / 
                         (self.performance_metrics['cache_hits'] + self.performance_metrics['cache_misses']) * 100) if (self.performance_metrics['cache_hits'] + self.performance_metrics['cache_misses']) > 0 else 0
        
        correlation_ids_count = len(self.performance_metrics['correlation_ids'])
        
        # RBAC analysis
        if self.rbac_test_results:
            rbac_success_count = sum(1 for result in self.rbac_test_results if result['success'])
            rbac_success_rate = (rbac_success_count / len(self.rbac_test_results)) * 100
        else:
            rbac_success_rate = 0
        
        # Final Results
        print("\n" + "=" * 80)
        print("üìä COMPREHENSIVE HARDENING TEST RESULTS")
        print("=" * 80)
        
        print(f"\nüéØ CRITICAL PERFORMANCE METRICS:")
        print(f"   P95 Latency: {p95_latency:.3f}s (Target: < 1.5s) {'‚úÖ' if p95_latency < 1.5 else '‚ùå'}")
        print(f"   Average Latency: {avg_latency:.3f}s")
        print(f"   Total Requests: {self.performance_metrics['total_requests']}")
        print(f"   5xx Errors: {self.performance_metrics['error_5xx_count']} (Target: 0) {'‚úÖ' if self.performance_metrics['error_5xx_count'] == 0 else '‚ùå'}")
        print(f"   Cache Hit Rate: {cache_hit_rate:.1f}% (Target: > 80%) {'‚úÖ' if cache_hit_rate > 80 else '‚ùå'}")
        print(f"   Correlation IDs: {correlation_ids_count}")
        
        print(f"\nüîê RBAC SECURITY METRICS:")
        print(f"   RBAC Success Rate: {rbac_success_rate:.1f}% (Target: 100%) {'‚úÖ' if rbac_success_rate >= 100 else '‚ùå'}")
        
        print(f"\nüìà TEST RESULTS SUMMARY:")
        print(f"   Tests Passed: {self.tests_passed}")
        print(f"   Tests Failed: {self.tests_run - self.tests_passed}")
        success_rate = (self.tests_passed/self.tests_run)*100 if self.tests_run > 0 else 0
        print(f"   Success Rate: {success_rate:.1f}%")
        
        # Critical success criteria
        critical_targets_met = (
            p95_latency < 1.5 and
            self.performance_metrics['error_5xx_count'] == 0 and
            rbac_success_rate >= 100.0 and
            cache_hit_rate > 80
        )
        
        print(f"\nüéØ PRODUCTION READINESS ASSESSMENT:")
        if critical_targets_met:
            print("üéâ PRODUCTION READY: All critical targets met!")
            print("‚úÖ P95 latency < 1.5s")
            print("‚úÖ Zero 5xx errors")
            print("‚úÖ RBAC security 100%")
            print("‚úÖ Cache performance > 80%")
        else:
            print("‚ùå NOT PRODUCTION READY: Critical targets not met")
            if p95_latency >= 1.5:
                print(f"‚ùå P95 latency {p95_latency:.3f}s exceeds 1.5s target")
            if self.performance_metrics['error_5xx_count'] > 0:
                print(f"‚ùå {self.performance_metrics['error_5xx_count']} 5xx errors detected")
            if rbac_success_rate < 100.0:
                print(f"‚ùå RBAC security {rbac_success_rate:.1f}% below 100% target")
            if cache_hit_rate <= 80:
                print(f"‚ùå Cache hit rate {cache_hit_rate:.1f}% below 80% target")
        
        return critical_targets_met

if __name__ == "__main__":
    tester = PerformanceHardeningTester()
    success = tester.run_comprehensive_hardening_test()
    sys.exit(0 if success else 1)